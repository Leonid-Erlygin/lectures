{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch\n",
    "from functools import reduce\n",
    "from typing import Union\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Let's have a cell with global hyperparameters for the CNNs in this notebook\n",
    "\n",
    "# Path to a directory with image dataset and subfolders for training, validation and final testing\n",
    "DATA_PATH = '../datasets' # PATH TO THE DATASET\n",
    "\n",
    "# Number of threads for data loader\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "# Image size: even though image sizes are bigger than 96, we use this to speed up training\n",
    "SIZE_H = SIZE_W = 224\n",
    "N_CHANNELS = 3\n",
    "\n",
    "# Number of classes in the dataset\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# Epochs: number of passes over the training data, we use it this small to reduce training babysitting time\n",
    "EPOCH_NUM = 30\n",
    "\n",
    "# Batch size: for batch gradient descent optimization, usually selected as 2**K elements\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Images mean and std channelwise\n",
    "image_mean = [0.485, 0.456, 0.406]\n",
    "image_std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Last layer (embeddings) size for CNN models\n",
    "EMBEDDING_SIZE = 256\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = transforms.Compose([\n",
    "    transforms.Resize((SIZE_H, SIZE_W)),        # scaling images to fixed size\n",
    "    transforms.ToTensor(),                      # converting to tensors\n",
    "    transforms.Lambda(lambda x: torch.cat([x, x, x], 0) if x.shape[0] == 1 else x),\n",
    "    transforms.Normalize(image_mean, image_std) # normalize image data per-channel\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caltech101 = torchvision.datasets.Caltech101(root=DATA_PATH, download=True, transform=transformer)#, transform=transformer)\n",
    "torch.manual_seed(0)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(caltech101, [7000, 1677])\n",
    "\n",
    "caltech101_unchanged = torchvision.datasets.Caltech101(root=DATA_PATH, download=True)\n",
    "torch.manual_seed(0)\n",
    "train_dataset_unchanged, val_dataset_unchanged = torch.utils.data.random_split(caltech101_unchanged, [7000, 1677])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train, n_val = len(train_dataset), len(val_dataset)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" VGG16\n",
    "    \"\"\"\n",
    "num_classes = 101\n",
    "model_ft = models.vgg16(pretrained=True)\n",
    "model_ft.classifier[6] = nn.Linear(model_ft.classifier[6].in_features, num_classes)\n",
    "model_ft.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, val_loader):\n",
    "    val_accuracy = []\n",
    "    for X_batch, y_batch in val_loader:\n",
    "            # move data to target device\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            # compute logits\n",
    "            logits = model(X_batch)\n",
    "            y_pred = logits.max(1)[1].data\n",
    "            val_accuracy.append(np.mean( (y_batch.cpu() == y_pred.cpu()).numpy() ))\n",
    "    return val_accuracy\n",
    "    \n",
    "def train_model(model, train_loader, val_loader, loss_fn, opt, n_epochs):\n",
    "    '''\n",
    "    model: нейросеть для обучения,\n",
    "    train_loader, val_loader: загрузчики данных\n",
    "    loss_fn: целевая метрика (которую будем оптимизировать)\n",
    "    opt: оптимизатор (обновляет веса нейросети)\n",
    "    n_epochs: кол-во эпох, полных проходов датасета\n",
    "    '''\n",
    "    train_loss = []\n",
    "    val_accuracy = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        model.train(True) # enable dropout / batch_norm training behavior\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            # move data to target device\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            # train on batch: compute loss, calc grads, perform optimizer step and zero the grads\n",
    "            opt.zero_grad()\n",
    "            predictions = model(X_batch)\n",
    "            loss = loss_fn(predictions, y_batch)\n",
    "            loss.backward()\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "            opt.step()\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        model.train(False) # disable dropout / use averages for batch_norm\n",
    "        val_accuracy += compute_accuracy(model, val_loader)\n",
    "\n",
    "        # print the results for this epoch:\n",
    "        print(f'Epoch {epoch + 1} of {n_epochs} took {time.time() - start_time:.3f}s')\n",
    "\n",
    "        train_loss_value = np.mean(train_loss[-n_train // BATCH_SIZE :])\n",
    "        val_accuracy_value = np.mean(val_accuracy[-n_val // BATCH_SIZE :]) * 100\n",
    "        \n",
    "        print(f\"  training loss (in-iteration): \\t{train_loss_value:.6f}\")\n",
    "        print(f\"  validation accuracy: \\t\\t\\t{val_accuracy_value:.2f} %\")\n",
    "\n",
    "    return train_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer_ft = torch.optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# train_loss, val_accuracy = train_model(model_ft,\n",
    "#                                          train_loader,\n",
    "#                                          val_loader,\n",
    "#                                          loss_fn,\n",
    "#                                          optimizer_ft,\n",
    "#                                          EPOCH_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model_ft.state_dict(), '../data/vgg16.pt')\n",
    "\n",
    "# load model\n",
    "\n",
    "num_classes = 101\n",
    "model_ft = models.vgg16(pretrained=True)\n",
    "model_ft.classifier[6] = nn.Linear(model_ft.classifier[6].in_features, num_classes)\n",
    "model_ft.to(device);\n",
    "model_ft.load_state_dict(torch.load('../data/vgg16.pt'))\n",
    "model_ft.eval();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(compute_accuracy(model_ft, val_loader))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nn_interpretability as nni\n",
    "from nn_interpretability.interpretation.cam.grad_cam import GradCAMInterpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_module_by_name(module: Union[torch.Tensor, nn.Module],\n",
    "                       access_string: str):\n",
    "    \"\"\"Retrieve a module nested in another by its access string.\n",
    "\n",
    "    Works even when there is a Sequential in the module.\n",
    "    \"\"\"\n",
    "    names = access_string.split(sep='.')\n",
    "    return reduce(getattr, names, module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize true predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\n",
    "    \"/home/devel/ws.leonid/lectures/l5_visualization/vis_tools/pytorch_cnn_visualizations/src\"\n",
    ")\n",
    "\n",
    "from nn_interpretability.interpretation.cam.grad_cam import GradCAMInterpreter\n",
    "from itertools import product\n",
    "from nn_interpretability.interpretation.backprop.guided_backprop import GuidedBackprop\n",
    "from nn_interpretability.visualization.rgb_visualizer import RGBVisualizer\n",
    "\n",
    "from nn_interpretability.interpretation.backprop.vanilla_backprop import VanillaBackprop\n",
    "from nn_interpretability.interpretation.backprop.guided_backprop import GuidedBackprop\n",
    "from nn_interpretability.interpretation.backprop.integrated_grad import IntegratedGrad\n",
    "from nn_interpretability.interpretation.backprop.smooth_grad import SmoothGrad\n",
    "from nn_interpretability.model.model_trainer import ModelTrainer\n",
    "from nn_interpretability.model.model_repository import ModelRepository\n",
    "from nn_interpretability.visualization.mnist_visualizer import MnistVisualizer\n",
    "from nn_interpretability.dataset.mnist_data_loader import MnistDataLoader\n",
    "\n",
    "from layercam import LayerCam\n",
    "from gradcam import GradCam\n",
    "from misc_functions import apply_colormap_on_image, apply_heatmap, convert_to_grayscale\n",
    "from LRP import LRP\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "def apply_heatmap(R):\n",
    "    \"\"\"\n",
    "    Heatmap code stolen from https://git.tu-berlin.de/gmontavon/lrp-tutorial\n",
    "\n",
    "    This is (so far) only used for LRP\n",
    "    \"\"\"\n",
    "    b = 10 * ((np.abs(R) ** 3.0).mean() ** (1.0 / 3))\n",
    "    my_cmap = plt.cm.seismic(np.arange(plt.cm.seismic.N))\n",
    "    my_cmap[:, 0:3] *= 0.85\n",
    "    my_cmap = ListedColormap(my_cmap)\n",
    "\n",
    "    # heatmap = plt.imshow(R, cmap=my_cmap, vmin=-b, vmax=b, interpolation='nearest')\n",
    "    # plt.show()\n",
    "    return my_cmap, b\n",
    "\n",
    "\n",
    "def get_explanations(\n",
    "    model,\n",
    "    image_transformed: torch.tensor,\n",
    "    image_unchanged: Image,\n",
    "    image_category_name: str,\n",
    "    image_category: int,\n",
    "    save_path: Path\n",
    "):\n",
    "\n",
    "    image_shape = image_unchanged.size\n",
    "\n",
    "    int_grad_steps = 20\n",
    "    smooth_grad_samples = 100\n",
    "    noise_level = 5\n",
    "    num_row = 5\n",
    "    num_col = 3\n",
    "\n",
    "    fig, axs = plt.subplots(num_row, num_col, figsize=(10, 15))\n",
    "    # set ticks to None\n",
    "    for i, j in product(range(num_row), range(num_col)):\n",
    "        axs[i, j].set_xticks([])\n",
    "        axs[i, j].set_yticks([])\n",
    "\n",
    "    # draw original image\n",
    "    axs[0, 0].imshow(image_unchanged)\n",
    "    axs[0, 0].set_title(f\"input image: {image_category_name}\")\n",
    "\n",
    "    # gradcam on image\n",
    "\n",
    "    grad_cam_extractor = GradCam(model_ft, target_layer=28)\n",
    "    grad_cam = cv2.resize(\n",
    "        grad_cam_extractor.generate_cam(image_transformed, target_class=image_category), image_shape\n",
    "    )\n",
    "\n",
    "    heatmap_grad_cam, heatmap_on_image_grad_cam = apply_colormap_on_image(\n",
    "        image_unchanged, grad_cam, \"hsv\"\n",
    "    )\n",
    "\n",
    "    axs[0, 1].imshow(heatmap_on_image_grad_cam)\n",
    "    axs[0, 1].set_title(\"GRAD-CAM on image\")\n",
    "\n",
    "    # gradcam\n",
    "    axs[0, 2].imshow(heatmap_grad_cam)\n",
    "    axs[0, 2].set_title(\"GRAD-CAM\")\n",
    "\n",
    "    # Vallina Backpropagation\n",
    "    interpretor = VanillaBackprop(model, [], None)\n",
    "    endpoint = interpretor.interpret(image_transformed, target_class = image_category)\n",
    "    gray = convert_to_grayscale(endpoint.detach().numpy()[0])\n",
    "    gray = gray - gray.min()\n",
    "    gray /= gray.max()\n",
    "    backprop_image = cv2.resize(\n",
    "        gray[0], image_shape\n",
    "    )\n",
    "    \n",
    "    axs[1, 0].imshow(backprop_image, interpolation=\"nearest\")\n",
    "    axs[1, 0].set_title(\"Vallina Backpropagation\")\n",
    "\n",
    "    # Guided Backpropagation\n",
    "    interpretor = GuidedBackprop(model_ft, [], None)\n",
    "    endpoint = interpretor.interpret(image_transformed, target_class = image_category)\n",
    "    guided_backprop = RGBVisualizer.postprocess(endpoint)\n",
    "    guided_backprop = cv2.resize(guided_backprop, image_shape)\n",
    "\n",
    "    axs[1, 1].imshow(guided_backprop, interpolation=\"nearest\")\n",
    "    axs[1, 1].set_title(\"Guided Backpropagation \")\n",
    "\n",
    "    # Integrated Gradients\n",
    "    baseline = torch.zeros_like(image_transformed)\n",
    "    interpretor = IntegratedGrad(model, [], None, baseline, steps=20)\n",
    "    endpoint = interpretor.interpret(image_transformed)\n",
    "    gray = convert_to_grayscale(endpoint.detach().numpy()[0])\n",
    "    gray = gray - gray.min()\n",
    "    gray /= gray.max()\n",
    "    int_grad_image = cv2.resize(\n",
    "        gray[0], image_shape\n",
    "    )\n",
    "    axs[1, 2].imshow(int_grad_image, interpolation=\"nearest\")\n",
    "    axs[1, 2].set_title(\"Integrated Gradients\")\n",
    "\n",
    "    # layer CAM. Layer 30\n",
    "\n",
    "    layer_cam_extractor = LayerCam(model_ft, target_layer=30)\n",
    "    layer_cam = cv2.resize(\n",
    "        layer_cam_extractor.generate_cam(image_transformed, target_class=image_category), image_shape\n",
    "    )\n",
    "\n",
    "    heatmap_layer_cam, heatmap_on_image_layer_cam = apply_colormap_on_image(\n",
    "        image_unchanged, layer_cam, \"hsv\"\n",
    "    )\n",
    "\n",
    "    axs[2, 0].imshow(heatmap_on_image_layer_cam)\n",
    "    axs[2, 0].set_title(\"Layer-CAM(Layer 30) on image\")\n",
    "\n",
    "    axs[3, 0].imshow(heatmap_layer_cam)\n",
    "    axs[3, 0].set_title(\"Layer-CAM(Layer 30)\")\n",
    "\n",
    "    # layer CAM. Layer 23\n",
    "\n",
    "    layer_cam_extractor = LayerCam(model_ft, target_layer=23)\n",
    "    layer_cam = cv2.resize(\n",
    "        layer_cam_extractor.generate_cam(image_transformed, target_class=image_category), image_shape\n",
    "    )\n",
    "\n",
    "    heatmap_layer_cam, heatmap_on_image_layer_cam = apply_colormap_on_image(\n",
    "        image_unchanged, layer_cam, \"hsv\"\n",
    "    )\n",
    "\n",
    "    axs[2, 1].imshow(heatmap_on_image_layer_cam)\n",
    "    axs[2, 1].set_title(\"Layer-CAM(Layer 23) on image\")\n",
    "\n",
    "    axs[3, 1].imshow(heatmap_layer_cam)\n",
    "    axs[3, 1].set_title(\"Layer-CAM(Layer 23)\")\n",
    "\n",
    "    # layer CAM. Layer 16\n",
    "\n",
    "    layer_cam_extractor = LayerCam(model_ft, target_layer=16)\n",
    "    layer_cam = cv2.resize(\n",
    "        layer_cam_extractor.generate_cam(image_transformed, target_class=image_category), image_shape\n",
    "    )\n",
    "\n",
    "    heatmap_layer_cam, heatmap_on_image_layer_cam = apply_colormap_on_image(\n",
    "        image_unchanged, layer_cam, \"hsv\"\n",
    "    )\n",
    "\n",
    "    axs[2, 2].imshow(heatmap_on_image_layer_cam)\n",
    "    axs[2, 2].set_title(\"Layer-CAM(Layer 16) on image\")\n",
    "\n",
    "    axs[3, 2].imshow(heatmap_layer_cam)\n",
    "    axs[3, 2].set_title(\"Layer-CAM(Layer 16)\")\n",
    "\n",
    "    # LRP. Layer 1\n",
    "    layer = 1\n",
    "    layerwise_relevance = LRP(model_ft)\n",
    "    LRP_per_layer = layerwise_relevance.generate(\n",
    "        image_transformed, target_class=image_category\n",
    "    )\n",
    "\n",
    "    # Convert the output nicely, selecting the first layer\n",
    "    lrp_to_vis = np.array(LRP_per_layer[layer][0].cpu()).sum(axis=0)\n",
    "    lrp_to_vis = np.array(\n",
    "        Image.fromarray(lrp_to_vis).resize(\n",
    "            (image_unchanged.size[0], image_unchanged.size[1]), Image.ANTIALIAS\n",
    "        )\n",
    "    )\n",
    "\n",
    "    my_cmap, b = apply_heatmap(lrp_to_vis)\n",
    "    axs[4, 0].imshow(lrp_to_vis, cmap=my_cmap, vmin=-b, vmax=b, interpolation=\"nearest\")\n",
    "    axs[4, 0].set_title(f\"LRP(Layer {layer})\")\n",
    "\n",
    "    # LRP. Layer 7\n",
    "    layer = 7\n",
    "    layerwise_relevance = LRP(model_ft)\n",
    "    LRP_per_layer = layerwise_relevance.generate(\n",
    "        image_transformed, target_class=image_category\n",
    "    )\n",
    "\n",
    "    # Convert the output nicely, selecting the first layer\n",
    "    lrp_to_vis = np.array(LRP_per_layer[layer][0].cpu()).sum(axis=0)\n",
    "    lrp_to_vis = np.array(\n",
    "        Image.fromarray(lrp_to_vis).resize(\n",
    "            (image_unchanged.size[0], image_unchanged.size[1]), Image.ANTIALIAS\n",
    "        )\n",
    "    )\n",
    "\n",
    "    my_cmap, b = apply_heatmap(lrp_to_vis)\n",
    "    axs[4, 1].imshow(lrp_to_vis, cmap=my_cmap, vmin=-b, vmax=b, interpolation=\"nearest\")\n",
    "    axs[4, 1].set_title(f\"LRP(Layer {layer})\")\n",
    "\n",
    "    # LRP. Layer 16\n",
    "    layer = 16\n",
    "    layerwise_relevance = LRP(model_ft)\n",
    "    LRP_per_layer = layerwise_relevance.generate(\n",
    "        image_transformed, target_class=image_category\n",
    "    )\n",
    "\n",
    "    # Convert the output nicely, selecting the first layer\n",
    "    lrp_to_vis = np.array(LRP_per_layer[layer][0].cpu()).sum(axis=0)\n",
    "    lrp_to_vis = np.array(\n",
    "        Image.fromarray(lrp_to_vis).resize(\n",
    "            (image_unchanged.size[0], image_unchanged.size[1]), Image.ANTIALIAS\n",
    "        )\n",
    "    )\n",
    "\n",
    "    my_cmap, b = apply_heatmap(lrp_to_vis)\n",
    "    axs[4, 2].imshow(lrp_to_vis, cmap=my_cmap, vmin=-b, vmax=b, interpolation=\"nearest\")\n",
    "    axs[4, 2].set_title(f\"LRP(Layer {layer})\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    save_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "    fig.savefig(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_index = 44\n",
    "image_unchanged, image_category = val_dataset_unchanged[image_index]\n",
    "image_transformed = torch.unsqueeze(transformer(image_unchanged), 0).to(device)\n",
    "get_explanations(model_ft, image_transformed, image_unchanged, caltech101.categories[image_category], image_category, Path('../outputs/test.png'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(5).zfill(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft.eval()\n",
    "\n",
    "for image_index in tqdm(range(len(val_dataset_unchanged))):\n",
    "    image_unchanged, image_category = val_dataset_unchanged[image_index]\n",
    "    true_category_name = caltech101.categories[image_category]\n",
    "    # prepare image\n",
    "    image_transformed = torch.unsqueeze(transformer(image_unchanged), 0).to(device)\n",
    "    # get class scores\n",
    "    class_scores = model_ft(image_transformed)\n",
    "    class_scores = class_scores.detach().cpu().numpy()[0]\n",
    "    predicted_class = np.argmax(class_scores)\n",
    "\n",
    "    formated_image_index = str(image_index).zfill(4)\n",
    "\n",
    "    if predicted_class == image_category:\n",
    "        # right classified image. Save its visualization to foulder with class category name\n",
    "        \n",
    "        save_path = Path(\n",
    "            f\"../outputs/val/true/{true_category_name}_id_{image_category}/{formated_image_index}.png\"\n",
    "        )\n",
    "        get_explanations(\n",
    "            model_ft,\n",
    "            image_transformed,\n",
    "            image_unchanged,\n",
    "            true_category_name,\n",
    "            image_category,\n",
    "            save_path,\n",
    "        )\n",
    "    else:\n",
    "        # misclassified image. Save vis with respect to true and predicted classes\n",
    "        predicted_category_name = caltech101.categories[predicted_class]\n",
    "        save_path_predicted_vis = Path(\n",
    "            f\"../outputs/val/mis_class/{predicted_category_name}_id_{predicted_class}_predicted/{formated_image_index}_predicted_category({predicted_category_name})_vis.png\"\n",
    "        )\n",
    "        save_path_true_vis = Path(\n",
    "            f\"../outputs/val/mis_class/{predicted_category_name}_id_{predicted_class}_predicted/{formated_image_index}_true_category({true_category_name})_vis.png\"\n",
    "        )\n",
    "\n",
    "        # predicted target vis\n",
    "        get_explanations(\n",
    "            model_ft,\n",
    "            image_transformed,\n",
    "            image_unchanged,\n",
    "            true_category_name,\n",
    "            predicted_class,\n",
    "            save_path_predicted_vis,\n",
    "        )\n",
    "        # true target vis\n",
    "        get_explanations(\n",
    "            model_ft,\n",
    "            image_transformed,\n",
    "            image_unchanged,\n",
    "            true_category_name,\n",
    "            image_category,\n",
    "            save_path_true_vis,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for module_pos, module in model_ft.features._modules.items():\n",
    "#     print(module_pos, module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for module_pos, module in model_ft.classifier._modules.items():\n",
    "    print(module_pos, module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model_ft, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bada2b9662873f86490f179f527b07823fa53d955971105c0ff3ba75e58f0801"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

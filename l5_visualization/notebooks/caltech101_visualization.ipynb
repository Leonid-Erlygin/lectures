{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch\n",
    "from functools import reduce\n",
    "from typing import Union\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "### Let's have a cell with global hyperparameters for the CNNs in this notebook\n",
    "\n",
    "# Path to a directory with image dataset and subfolders for training, validation and final testing\n",
    "DATA_PATH = '../datasets' # PATH TO THE DATASET\n",
    "\n",
    "# Number of threads for data loader\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "# Image size: even though image sizes are bigger than 96, we use this to speed up training\n",
    "SIZE_H = SIZE_W = 224\n",
    "N_CHANNELS = 3\n",
    "\n",
    "# Number of classes in the dataset\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# Epochs: number of passes over the training data, we use it this small to reduce training babysitting time\n",
    "EPOCH_NUM = 30\n",
    "\n",
    "# Batch size: for batch gradient descent optimization, usually selected as 2**K elements\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Images mean and std channelwise\n",
    "image_mean = [0.485, 0.456, 0.406]\n",
    "image_std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Last layer (embeddings) size for CNN models\n",
    "EMBEDDING_SIZE = 256\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = transforms.Compose([\n",
    "    transforms.Resize((SIZE_H, SIZE_W)),        # scaling images to fixed size\n",
    "    transforms.ToTensor(),                      # converting to tensors\n",
    "    transforms.Lambda(lambda x: torch.cat([x, x, x], 0) if x.shape[0] == 1 else x),\n",
    "    transforms.Normalize(image_mean, image_std) # normalize image data per-channel\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "caltech101 = torchvision.datasets.Caltech101(root=DATA_PATH, download=True, transform=transformer)#, transform=transformer)\n",
    "torch.manual_seed(0)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(caltech101, [7000, 1677])\n",
    "\n",
    "caltech101_unchanged = torchvision.datasets.Caltech101(root=DATA_PATH, download=True)\n",
    "torch.manual_seed(0)\n",
    "train_dataset_unchanged, val_dataset_unchanged = torch.utils.data.random_split(caltech101_unchanged, [7000, 1677])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train, n_val = len(train_dataset), len(val_dataset)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devel/ws.leonid/lectures/.venv/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/devel/ws.leonid/lectures/.venv/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" VGG16\n",
    "    \"\"\"\n",
    "num_classes = 101\n",
    "model_ft = models.vgg16(pretrained=True)\n",
    "model_ft.classifier[6] = nn.Linear(model_ft.classifier[6].in_features, num_classes)\n",
    "model_ft.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, val_loader):\n",
    "    val_accuracy = []\n",
    "    for X_batch, y_batch in val_loader:\n",
    "            # move data to target device\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            # compute logits\n",
    "            logits = model(X_batch)\n",
    "            y_pred = logits.max(1)[1].data\n",
    "            val_accuracy.append(np.mean( (y_batch.cpu() == y_pred.cpu()).numpy() ))\n",
    "    return val_accuracy\n",
    "    \n",
    "def train_model(model, train_loader, val_loader, loss_fn, opt, n_epochs):\n",
    "    '''\n",
    "    model: нейросеть для обучения,\n",
    "    train_loader, val_loader: загрузчики данных\n",
    "    loss_fn: целевая метрика (которую будем оптимизировать)\n",
    "    opt: оптимизатор (обновляет веса нейросети)\n",
    "    n_epochs: кол-во эпох, полных проходов датасета\n",
    "    '''\n",
    "    train_loss = []\n",
    "    val_accuracy = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        model.train(True) # enable dropout / batch_norm training behavior\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            # move data to target device\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            # train on batch: compute loss, calc grads, perform optimizer step and zero the grads\n",
    "            opt.zero_grad()\n",
    "            predictions = model(X_batch)\n",
    "            loss = loss_fn(predictions, y_batch)\n",
    "            loss.backward()\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "            opt.step()\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        model.train(False) # disable dropout / use averages for batch_norm\n",
    "        val_accuracy += compute_accuracy(model, val_loader)\n",
    "\n",
    "        # print the results for this epoch:\n",
    "        print(f'Epoch {epoch + 1} of {n_epochs} took {time.time() - start_time:.3f}s')\n",
    "\n",
    "        train_loss_value = np.mean(train_loss[-n_train // BATCH_SIZE :])\n",
    "        val_accuracy_value = np.mean(val_accuracy[-n_val // BATCH_SIZE :]) * 100\n",
    "        \n",
    "        print(f\"  training loss (in-iteration): \\t{train_loss_value:.6f}\")\n",
    "        print(f\"  validation accuracy: \\t\\t\\t{val_accuracy_value:.2f} %\")\n",
    "\n",
    "    return train_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m optimizer_ft \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mSGD(model_ft\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m, momentum\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m)\n\u001b[1;32m      2\u001b[0m loss_fn \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m----> 3\u001b[0m train_loss, val_accuracy \u001b[39m=\u001b[39m train_model(model_ft,\n\u001b[1;32m      4\u001b[0m                                          train_loader,\n\u001b[1;32m      5\u001b[0m                                          val_loader,\n\u001b[1;32m      6\u001b[0m                                          loss_fn,\n\u001b[1;32m      7\u001b[0m                                          optimizer_ft,\n\u001b[1;32m      8\u001b[0m                                          EPOCH_NUM)\n",
      "Cell \u001b[0;32mIn [7], line 37\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, loss_fn, opt, n_epochs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39m#             torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m             opt\u001b[39m.\u001b[39mstep()\n\u001b[0;32m---> 37\u001b[0m             train_loss\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39;49mitem())\n\u001b[1;32m     39\u001b[0m         model\u001b[39m.\u001b[39mtrain(\u001b[39mFalse\u001b[39;00m) \u001b[39m# disable dropout / use averages for batch_norm\u001b[39;00m\n\u001b[1;32m     40\u001b[0m         val_accuracy \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m compute_accuracy(model, val_loader)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# optimizer_ft = torch.optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# train_loss, val_accuracy = train_model(model_ft,\n",
    "#                                          train_loader,\n",
    "#                                          val_loader,\n",
    "#                                          loss_fn,\n",
    "#                                          optimizer_ft,\n",
    "#                                          EPOCH_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model_ft.state_dict(), '../data/vgg16.pt')\n",
    "\n",
    "# load model\n",
    "\n",
    "num_classes = 101\n",
    "model_ft = models.vgg16(pretrained=True)\n",
    "model_ft.classifier[6] = nn.Linear(model_ft.classifier[6].in_features, num_classes)\n",
    "model_ft.to(device);\n",
    "model_ft.load_state_dict(torch.load('../data/vgg16.pt'))\n",
    "model_ft.eval();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94.81585631349783"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(compute_accuracy(model_ft, val_loader))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nn_interpretability as nni\n",
    "from nn_interpretability.interpretation.cam.grad_cam import GradCAMInterpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_module_by_name(module: Union[torch.Tensor, nn.Module],\n",
    "                       access_string: str):\n",
    "    \"\"\"Retrieve a module nested in another by its access string.\n",
    "\n",
    "    Works even when there is a Sequential in the module.\n",
    "    \"\"\"\n",
    "    names = access_string.split(sep='.')\n",
    "    return reduce(getattr, names, module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize true predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\n",
    "    \"/home/devel/ws.leonid/lectures/l5_visualization/vis_tools/pytorch_cnn_visualizations/src\"\n",
    ")\n",
    "\n",
    "from nn_interpretability.interpretation.cam.grad_cam import GradCAMInterpreter\n",
    "from itertools import product\n",
    "from nn_interpretability.interpretation.backprop.guided_backprop import GuidedBackprop\n",
    "from nn_interpretability.visualization.rgb_visualizer import RGBVisualizer\n",
    "\n",
    "from nn_interpretability.interpretation.backprop.vanilla_backprop import VanillaBackprop\n",
    "from nn_interpretability.interpretation.backprop.guided_backprop import GuidedBackprop\n",
    "from nn_interpretability.interpretation.backprop.integrated_grad import IntegratedGrad\n",
    "from nn_interpretability.interpretation.backprop.smooth_grad import SmoothGrad\n",
    "from nn_interpretability.model.model_trainer import ModelTrainer\n",
    "from nn_interpretability.model.model_repository import ModelRepository\n",
    "from nn_interpretability.visualization.mnist_visualizer import MnistVisualizer\n",
    "from nn_interpretability.dataset.mnist_data_loader import MnistDataLoader\n",
    "\n",
    "from layercam import LayerCam\n",
    "from gradcam import GradCam\n",
    "from misc_functions import apply_colormap_on_image, apply_heatmap, convert_to_grayscale\n",
    "from LRP import LRP\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "def apply_heatmap(R):\n",
    "    \"\"\"\n",
    "    Heatmap code stolen from https://git.tu-berlin.de/gmontavon/lrp-tutorial\n",
    "\n",
    "    This is (so far) only used for LRP\n",
    "    \"\"\"\n",
    "    b = 10 * ((np.abs(R) ** 3.0).mean() ** (1.0 / 3))\n",
    "    my_cmap = plt.cm.seismic(np.arange(plt.cm.seismic.N))\n",
    "    my_cmap[:, 0:3] *= 0.85\n",
    "    my_cmap = ListedColormap(my_cmap)\n",
    "\n",
    "    # heatmap = plt.imshow(R, cmap=my_cmap, vmin=-b, vmax=b, interpolation='nearest')\n",
    "    # plt.show()\n",
    "    return my_cmap, b\n",
    "\n",
    "\n",
    "def get_explanations(\n",
    "    model,\n",
    "    image_transformed: torch.tensor,\n",
    "    image_unchanged: Image,\n",
    "    image_category_name: str,\n",
    "    image_category: int,\n",
    "    save_path: Path\n",
    "):\n",
    "\n",
    "    image_shape = image_unchanged.size\n",
    "\n",
    "    int_grad_steps = 10\n",
    "    num_row = 5\n",
    "    num_col = 3\n",
    "\n",
    "    fig, axs = plt.subplots(num_row, num_col, figsize=(10, 15))\n",
    "    # set ticks to None\n",
    "    for i, j in product(range(num_row), range(num_col)):\n",
    "        axs[i, j].set_xticks([])\n",
    "        axs[i, j].set_yticks([])\n",
    "\n",
    "    # draw original image\n",
    "    axs[0, 0].imshow(image_unchanged)\n",
    "    axs[0, 0].set_title(f\"input image: {image_category_name}\")\n",
    "\n",
    "    # gradcam on image\n",
    "\n",
    "    grad_cam_extractor = GradCam(model_ft, target_layer=28)\n",
    "    grad_cam = cv2.resize(\n",
    "        grad_cam_extractor.generate_cam(image_transformed, target_class=image_category), image_shape\n",
    "    )\n",
    "\n",
    "    heatmap_grad_cam, heatmap_on_image_grad_cam = apply_colormap_on_image(\n",
    "        image_unchanged, grad_cam, \"hsv\"\n",
    "    )\n",
    "\n",
    "    axs[0, 1].imshow(heatmap_on_image_grad_cam)\n",
    "    axs[0, 1].set_title(\"GRAD-CAM on image\")\n",
    "\n",
    "    # gradcam\n",
    "    axs[0, 2].imshow(heatmap_grad_cam)\n",
    "    axs[0, 2].set_title(\"GRAD-CAM\")\n",
    "\n",
    "    # Vallina Backpropagation\n",
    "    interpretor = VanillaBackprop(model, [], None)\n",
    "    endpoint = interpretor.interpret(image_transformed, target_class = image_category)\n",
    "    gray = convert_to_grayscale(endpoint.detach().numpy()[0])\n",
    "    gray = gray - gray.min()\n",
    "    gray /= gray.max()\n",
    "    backprop_image = cv2.resize(\n",
    "        gray[0], image_shape\n",
    "    )\n",
    "    backprop_image = (backprop_image * 255).astype('uint8')\n",
    "    axs[1, 0].imshow(backprop_image, interpolation=\"nearest\")\n",
    "    axs[1, 0].set_title(\"Vallina Backpropagation\")\n",
    "\n",
    "    # Guided Backpropagation\n",
    "    interpretor = GuidedBackprop(model_ft, [], None)\n",
    "    endpoint = interpretor.interpret(image_transformed, target_class = image_category)\n",
    "    guided_backprop = RGBVisualizer.postprocess(endpoint)\n",
    "    guided_backprop = cv2.resize(guided_backprop, image_shape)\n",
    "\n",
    "    guided_backprop = (guided_backprop * 255).astype('uint8')\n",
    "    axs[1, 1].imshow(guided_backprop, interpolation=\"nearest\")\n",
    "    axs[1, 1].set_title(\"Guided Backpropagation \")\n",
    "\n",
    "    # Integrated Gradients\n",
    "    baseline = torch.zeros_like(image_transformed)\n",
    "    interpretor = IntegratedGrad(model, [], None, baseline, steps=20)\n",
    "    endpoint = interpretor.interpret(image_transformed)\n",
    "    gray = convert_to_grayscale(endpoint.detach().numpy()[0])\n",
    "    gray = gray - gray.min()\n",
    "    gray /= gray.max()\n",
    "    int_grad_image = cv2.resize(\n",
    "        gray[0], image_shape\n",
    "    )\n",
    "\n",
    "    int_grad_image = (int_grad_image * 255).astype('uint8')\n",
    "    axs[1, 2].imshow(int_grad_image, interpolation=\"nearest\")\n",
    "    axs[1, 2].set_title(\"Integrated Gradients\")\n",
    "\n",
    "    # layer CAM. Layer 30\n",
    "\n",
    "    layer_cam_extractor = LayerCam(model_ft, target_layer=30)\n",
    "    layer_cam = cv2.resize(\n",
    "        layer_cam_extractor.generate_cam(image_transformed, target_class=image_category), image_shape\n",
    "    )\n",
    "\n",
    "    heatmap_layer_cam, heatmap_on_image_layer_cam = apply_colormap_on_image(\n",
    "        image_unchanged, layer_cam, \"hsv\"\n",
    "    )\n",
    "\n",
    "    axs[2, 0].imshow(heatmap_on_image_layer_cam)\n",
    "    axs[2, 0].set_title(\"Layer-CAM(Layer 30) on image\")\n",
    "\n",
    "    axs[3, 0].imshow(heatmap_layer_cam)\n",
    "    axs[3, 0].set_title(\"Layer-CAM(Layer 30)\")\n",
    "\n",
    "    # layer CAM. Layer 23\n",
    "\n",
    "    layer_cam_extractor = LayerCam(model_ft, target_layer=23)\n",
    "    layer_cam = cv2.resize(\n",
    "        layer_cam_extractor.generate_cam(image_transformed, target_class=image_category), image_shape\n",
    "    )\n",
    "\n",
    "    heatmap_layer_cam, heatmap_on_image_layer_cam = apply_colormap_on_image(\n",
    "        image_unchanged, layer_cam, \"hsv\"\n",
    "    )\n",
    "\n",
    "    axs[2, 1].imshow(heatmap_on_image_layer_cam)\n",
    "    axs[2, 1].set_title(\"Layer-CAM(Layer 23) on image\")\n",
    "\n",
    "    axs[3, 1].imshow(heatmap_layer_cam)\n",
    "    axs[3, 1].set_title(\"Layer-CAM(Layer 23)\")\n",
    "\n",
    "    # layer CAM. Layer 16\n",
    "\n",
    "    layer_cam_extractor = LayerCam(model_ft, target_layer=16)\n",
    "    layer_cam = cv2.resize(\n",
    "        layer_cam_extractor.generate_cam(image_transformed, target_class=image_category), image_shape\n",
    "    )\n",
    "\n",
    "    heatmap_layer_cam, heatmap_on_image_layer_cam = apply_colormap_on_image(\n",
    "        image_unchanged, layer_cam, \"hsv\"\n",
    "    )\n",
    "\n",
    "    axs[2, 2].imshow(heatmap_on_image_layer_cam)\n",
    "    axs[2, 2].set_title(\"Layer-CAM(Layer 16) on image\")\n",
    "\n",
    "    axs[3, 2].imshow(heatmap_layer_cam)\n",
    "    axs[3, 2].set_title(\"Layer-CAM(Layer 16)\")\n",
    "\n",
    "    # LRP. Layer 1\n",
    "    layer = 1\n",
    "    layerwise_relevance = LRP(model_ft)\n",
    "    LRP_per_layer = layerwise_relevance.generate(\n",
    "        image_transformed, target_class=image_category\n",
    "    )\n",
    "\n",
    "    # Convert the output nicely, selecting the first layer\n",
    "    lrp_to_vis = np.array(LRP_per_layer[layer][0].cpu()).sum(axis=0)\n",
    "    lrp_to_vis = np.array(\n",
    "        Image.fromarray(lrp_to_vis).resize(\n",
    "            (image_unchanged.size[0], image_unchanged.size[1]), Image.ANTIALIAS\n",
    "        )\n",
    "    )\n",
    "\n",
    "    my_cmap, b = apply_heatmap(lrp_to_vis)\n",
    "    axs[4, 0].imshow(lrp_to_vis, cmap=my_cmap, vmin=-b, vmax=b, interpolation=\"nearest\")\n",
    "    axs[4, 0].set_title(f\"LRP(Layer {layer})\")\n",
    "\n",
    "    # LRP. Layer 7\n",
    "    layer = 7\n",
    "    layerwise_relevance = LRP(model_ft)\n",
    "    LRP_per_layer = layerwise_relevance.generate(\n",
    "        image_transformed, target_class=image_category\n",
    "    )\n",
    "\n",
    "    # Convert the output nicely, selecting the first layer\n",
    "    lrp_to_vis = np.array(LRP_per_layer[layer][0].cpu()).sum(axis=0)\n",
    "    lrp_to_vis = np.array(\n",
    "        Image.fromarray(lrp_to_vis).resize(\n",
    "            (image_unchanged.size[0], image_unchanged.size[1]), Image.ANTIALIAS\n",
    "        )\n",
    "    )\n",
    "\n",
    "    my_cmap, b = apply_heatmap(lrp_to_vis)\n",
    "    axs[4, 1].imshow(lrp_to_vis, cmap=my_cmap, vmin=-b, vmax=b, interpolation=\"nearest\")\n",
    "    axs[4, 1].set_title(f\"LRP(Layer {layer})\")\n",
    "\n",
    "    # LRP. Layer 16\n",
    "    layer = 16\n",
    "    layerwise_relevance = LRP(model_ft)\n",
    "    LRP_per_layer = layerwise_relevance.generate(\n",
    "        image_transformed, target_class=image_category\n",
    "    )\n",
    "\n",
    "    # Convert the output nicely, selecting the first layer\n",
    "    lrp_to_vis = np.array(LRP_per_layer[layer][0].cpu()).sum(axis=0)\n",
    "    lrp_to_vis = np.array(\n",
    "        Image.fromarray(lrp_to_vis).resize(\n",
    "            (image_unchanged.size[0], image_unchanged.size[1]), Image.ANTIALIAS\n",
    "        )\n",
    "    )\n",
    "\n",
    "    my_cmap, b = apply_heatmap(lrp_to_vis)\n",
    "    axs[4, 2].imshow(lrp_to_vis, cmap=my_cmap, vmin=-b, vmax=b, interpolation=\"nearest\")\n",
    "    axs[4, 2].set_title(f\"LRP(Layer {layer})\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    save_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "    fig.savefig(save_path)\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devel/ws.leonid/lectures/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    }
   ],
   "source": [
    "image_index = 44\n",
    "image_unchanged, image_category = val_dataset_unchanged[image_index]\n",
    "image_transformed = torch.unsqueeze(transformer(image_unchanged), 0).to(device)\n",
    "get_explanations(model_ft, image_transformed, image_unchanged, caltech101.categories[image_category], image_category, Path('../outputs/test.png'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 36/1677 [00:57<43:59,  1.61s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [24], line 21\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mif\u001b[39;00m predicted_class \u001b[39m==\u001b[39m image_category:\n\u001b[1;32m     16\u001b[0m     \u001b[39m# right classified image. Save its visualization to foulder with class category name\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     save_path \u001b[39m=\u001b[39mout_dir \u001b[39m/\u001b[39m Path(\n\u001b[1;32m     19\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrue/\u001b[39m\u001b[39m{\u001b[39;00mtrue_category_name\u001b[39m}\u001b[39;00m\u001b[39m_id_\u001b[39m\u001b[39m{\u001b[39;00mimage_category\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mformated_image_index\u001b[39m}\u001b[39;00m\u001b[39m.png\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m     )\n\u001b[0;32m---> 21\u001b[0m     get_explanations(\n\u001b[1;32m     22\u001b[0m         model_ft,\n\u001b[1;32m     23\u001b[0m         image_transformed,\n\u001b[1;32m     24\u001b[0m         image_unchanged,\n\u001b[1;32m     25\u001b[0m         true_category_name,\n\u001b[1;32m     26\u001b[0m         image_category,\n\u001b[1;32m     27\u001b[0m         save_path,\n\u001b[1;32m     28\u001b[0m     )\n\u001b[1;32m     29\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     30\u001b[0m     \u001b[39m# misclassified image. Save vis with respect to true and predicted classes\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     predicted_category_name \u001b[39m=\u001b[39m caltech101\u001b[39m.\u001b[39mcategories[predicted_class]\n",
      "Cell \u001b[0;32mIn [18], line 62\u001b[0m, in \u001b[0;36mget_explanations\u001b[0;34m(model, image_transformed, image_unchanged, image_category_name, image_category, save_path)\u001b[0m\n\u001b[1;32m     59\u001b[0m num_row \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n\u001b[1;32m     60\u001b[0m num_col \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[0;32m---> 62\u001b[0m fig, axs \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39;49msubplots(num_row, num_col, figsize\u001b[39m=\u001b[39;49m(\u001b[39m10\u001b[39;49m, \u001b[39m15\u001b[39;49m))\n\u001b[1;32m     63\u001b[0m \u001b[39m# set ticks to None\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mfor\u001b[39;00m i, j \u001b[39min\u001b[39;00m product(\u001b[39mrange\u001b[39m(num_row), \u001b[39mrange\u001b[39m(num_col)):\n",
      "File \u001b[0;32m~/ws.leonid/lectures/.venv/lib/python3.8/site-packages/matplotlib/cbook/deprecation.py:451\u001b[0m, in \u001b[0;36m_make_keyword_only.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m idx:\n\u001b[1;32m    446\u001b[0m     warn_deprecated(\n\u001b[1;32m    447\u001b[0m         since, message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPassing the \u001b[39m\u001b[39m%(name)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%(obj_type)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpositionally is deprecated since Matplotlib \u001b[39m\u001b[39m%(since)s\u001b[39;00m\u001b[39m; the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    449\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mparameter will become keyword-only \u001b[39m\u001b[39m%(removal)s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    450\u001b[0m         name\u001b[39m=\u001b[39mname, obj_type\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 451\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/ws.leonid/lectures/.venv/lib/python3.8/site-packages/matplotlib/pyplot.py:1271\u001b[0m, in \u001b[0;36msubplots\u001b[0;34m(nrows, ncols, sharex, sharey, squeeze, subplot_kw, gridspec_kw, **fig_kw)\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1143\u001b[0m \u001b[39mCreate a figure and a set of subplots.\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1268\u001b[0m \n\u001b[1;32m   1269\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1270\u001b[0m fig \u001b[39m=\u001b[39m figure(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfig_kw)\n\u001b[0;32m-> 1271\u001b[0m axs \u001b[39m=\u001b[39m fig\u001b[39m.\u001b[39;49msubplots(nrows\u001b[39m=\u001b[39;49mnrows, ncols\u001b[39m=\u001b[39;49mncols, sharex\u001b[39m=\u001b[39;49msharex, sharey\u001b[39m=\u001b[39;49msharey,\n\u001b[1;32m   1272\u001b[0m                    squeeze\u001b[39m=\u001b[39;49msqueeze, subplot_kw\u001b[39m=\u001b[39;49msubplot_kw,\n\u001b[1;32m   1273\u001b[0m                    gridspec_kw\u001b[39m=\u001b[39;49mgridspec_kw)\n\u001b[1;32m   1274\u001b[0m \u001b[39mreturn\u001b[39;00m fig, axs\n",
      "File \u001b[0;32m~/ws.leonid/lectures/.venv/lib/python3.8/site-packages/matplotlib/cbook/deprecation.py:451\u001b[0m, in \u001b[0;36m_make_keyword_only.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m idx:\n\u001b[1;32m    446\u001b[0m     warn_deprecated(\n\u001b[1;32m    447\u001b[0m         since, message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPassing the \u001b[39m\u001b[39m%(name)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%(obj_type)s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpositionally is deprecated since Matplotlib \u001b[39m\u001b[39m%(since)s\u001b[39;00m\u001b[39m; the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    449\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mparameter will become keyword-only \u001b[39m\u001b[39m%(removal)s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    450\u001b[0m         name\u001b[39m=\u001b[39mname, obj_type\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 451\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/ws.leonid/lectures/.venv/lib/python3.8/site-packages/matplotlib/figure.py:1522\u001b[0m, in \u001b[0;36mFigure.subplots\u001b[0;34m(self, nrows, ncols, sharex, sharey, squeeze, subplot_kw, gridspec_kw)\u001b[0m\n\u001b[1;32m   1520\u001b[0m \u001b[39mif\u001b[39;00m gridspec_kw \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1521\u001b[0m     gridspec_kw \u001b[39m=\u001b[39m {}\n\u001b[0;32m-> 1522\u001b[0m \u001b[39mreturn\u001b[39;00m (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_gridspec(nrows, ncols, figure\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgridspec_kw)\n\u001b[1;32m   1523\u001b[0m         \u001b[39m.\u001b[39;49msubplots(sharex\u001b[39m=\u001b[39;49msharex, sharey\u001b[39m=\u001b[39;49msharey, squeeze\u001b[39m=\u001b[39;49msqueeze,\n\u001b[1;32m   1524\u001b[0m                   subplot_kw\u001b[39m=\u001b[39;49msubplot_kw))\n",
      "File \u001b[0;32m~/ws.leonid/lectures/.venv/lib/python3.8/site-packages/matplotlib/gridspec.py:336\u001b[0m, in \u001b[0;36mGridSpecBase.subplots\u001b[0;34m(self, sharex, sharey, squeeze, subplot_kw)\u001b[0m\n\u001b[1;32m    334\u001b[0m         subplot_kw[\u001b[39m\"\u001b[39m\u001b[39msharex\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m shared_with[sharex]\n\u001b[1;32m    335\u001b[0m         subplot_kw[\u001b[39m\"\u001b[39m\u001b[39msharey\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m shared_with[sharey]\n\u001b[0;32m--> 336\u001b[0m         axarr[row, col] \u001b[39m=\u001b[39m figure\u001b[39m.\u001b[39;49madd_subplot(\n\u001b[1;32m    337\u001b[0m             \u001b[39mself\u001b[39;49m[row, col], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msubplot_kw)\n\u001b[1;32m    339\u001b[0m \u001b[39m# turn off redundant tick labeling\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[39mif\u001b[39;00m sharex \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mcol\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    341\u001b[0m     \u001b[39m# turn off all but the bottom row\u001b[39;00m\n",
      "File \u001b[0;32m~/ws.leonid/lectures/.venv/lib/python3.8/site-packages/matplotlib/figure.py:1402\u001b[0m, in \u001b[0;36mFigure.add_subplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1395\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1396\u001b[0m             \u001b[39m# Undocumented convenience behavior:\u001b[39;00m\n\u001b[1;32m   1397\u001b[0m             \u001b[39m# subplot(111); subplot(111, projection='polar')\u001b[39;00m\n\u001b[1;32m   1398\u001b[0m             \u001b[39m# will replace the first with the second.\u001b[39;00m\n\u001b[1;32m   1399\u001b[0m             \u001b[39m# Without this, add_subplot would be simpler and\u001b[39;00m\n\u001b[1;32m   1400\u001b[0m             \u001b[39m# more similar to add_axes.\u001b[39;00m\n\u001b[1;32m   1401\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_axstack\u001b[39m.\u001b[39mremove(ax)\n\u001b[0;32m-> 1402\u001b[0m     ax \u001b[39m=\u001b[39m subplot_class_factory(projection_class)(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1404\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_add_axes_internal(key, ax)\n",
      "File \u001b[0;32m~/ws.leonid/lectures/.venv/lib/python3.8/site-packages/matplotlib/axes/_subplots.py:42\u001b[0m, in \u001b[0;36mSubplotBase.__init__\u001b[0;34m(self, fig, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_params()\n\u001b[1;32m     41\u001b[0m \u001b[39m# _axes_class is set in the subplot_class_factory\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_axes_class\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39mself\u001b[39;49m, fig, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfigbox, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     43\u001b[0m \u001b[39m# add a layout box to this, for both the full axis, and the poss\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39m# of the axis.  We need both because the axes may become smaller\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[39m# due to parasitic axes and hence no longer fill the subplotspec.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_subplotspec\u001b[39m.\u001b[39m_layoutbox \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/ws.leonid/lectures/.venv/lib/python3.8/site-packages/matplotlib/axes/_base.py:500\u001b[0m, in \u001b[0;36m_AxesBase.__init__\u001b[0;34m(self, fig, rect, facecolor, frameon, sharex, sharey, label, xscale, yscale, box_aspect, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_box_aspect(box_aspect)\n\u001b[1;32m    498\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_axes_locator \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# Optionally set via update(kwargs).\u001b[39;00m\n\u001b[0;32m--> 500\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspines \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gen_axes_spines()\n\u001b[1;32m    502\u001b[0m \u001b[39m# this call may differ for non-sep axes, e.g., polar\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_axis()\n",
      "File \u001b[0;32m~/ws.leonid/lectures/.venv/lib/python3.8/site-packages/matplotlib/axes/_base.py:1002\u001b[0m, in \u001b[0;36m_AxesBase._gen_axes_spines\u001b[0;34m(self, locations, offset, units)\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_gen_axes_spines\u001b[39m(\u001b[39mself\u001b[39m, locations\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, offset\u001b[39m=\u001b[39m\u001b[39m0.0\u001b[39m, units\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39minches\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    988\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[39m    Returns\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[39m    -------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1000\u001b[0m \u001b[39m    Intended to be overridden by new projection types.\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1002\u001b[0m     \u001b[39mreturn\u001b[39;00m OrderedDict((side, mspines\u001b[39m.\u001b[39;49mSpine\u001b[39m.\u001b[39;49mlinear_spine(\u001b[39mself\u001b[39;49m, side))\n\u001b[1;32m   1003\u001b[0m                        \u001b[39mfor\u001b[39;49;00m side \u001b[39min\u001b[39;49;00m [\u001b[39m'\u001b[39;49m\u001b[39mleft\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mright\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mbottom\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mtop\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[0;32m~/ws.leonid/lectures/.venv/lib/python3.8/site-packages/matplotlib/axes/_base.py:1002\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_gen_axes_spines\u001b[39m(\u001b[39mself\u001b[39m, locations\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, offset\u001b[39m=\u001b[39m\u001b[39m0.0\u001b[39m, units\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39minches\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    988\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[39m    Returns\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[39m    -------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1000\u001b[0m \u001b[39m    Intended to be overridden by new projection types.\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1002\u001b[0m     \u001b[39mreturn\u001b[39;00m OrderedDict((side, mspines\u001b[39m.\u001b[39;49mSpine\u001b[39m.\u001b[39;49mlinear_spine(\u001b[39mself\u001b[39;49m, side))\n\u001b[1;32m   1003\u001b[0m                        \u001b[39mfor\u001b[39;00m side \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mright\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbottom\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtop\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/ws.leonid/lectures/.venv/lib/python3.8/site-packages/matplotlib/spines.py:498\u001b[0m, in \u001b[0;36mSpine.linear_spine\u001b[0;34m(cls, axes, spine_type, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    497\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39munable to make path for spine \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m spine_type)\n\u001b[0;32m--> 498\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(axes, spine_type, path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    499\u001b[0m result\u001b[39m.\u001b[39mset_visible(rcParams[\u001b[39m'\u001b[39m\u001b[39maxes.spines.\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(spine_type)])\n\u001b[1;32m    501\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/ws.leonid/lectures/.venv/lib/python3.8/site-packages/matplotlib/spines.py:50\u001b[0m, in \u001b[0;36mSpine.__init__\u001b[0;34m(self, axes, spine_type, path, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39m@docstring\u001b[39m\u001b[39m.\u001b[39mdedent_interpd\n\u001b[1;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, axes, spine_type, path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     33\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m    ----------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39m        %(Patch)s\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     51\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes \u001b[39m=\u001b[39m axes\n\u001b[1;32m     52\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_figure(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes\u001b[39m.\u001b[39mfigure)\n",
      "File \u001b[0;32m~/ws.leonid/lectures/.venv/lib/python3.8/site-packages/matplotlib/patches.py:81\u001b[0m, in \u001b[0;36mPatch.__init__\u001b[0;34m(self, edgecolor, facecolor, color, linewidth, linestyle, antialiased, hatch, fill, capstyle, joinstyle, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_edgecolor(edgecolor)\n\u001b[0;32m---> 81\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset_facecolor(facecolor)\n\u001b[1;32m     82\u001b[0m \u001b[39m# unscaled dashes.  Needed to scale dash patterns by lw\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_us_dashes \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/ws.leonid/lectures/.venv/lib/python3.8/site-packages/matplotlib/patches.py:355\u001b[0m, in \u001b[0;36mPatch.set_facecolor\u001b[0;34m(self, color)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[39mSet the patch face color.\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[39mcolor : color or None\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_facecolor \u001b[39m=\u001b[39m color\n\u001b[0;32m--> 355\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_facecolor(color)\n",
      "File \u001b[0;32m~/ws.leonid/lectures/.venv/lib/python3.8/site-packages/matplotlib/patches.py:343\u001b[0m, in \u001b[0;36mPatch._set_facecolor\u001b[0;34m(self, color)\u001b[0m\n\u001b[1;32m    341\u001b[0m     color \u001b[39m=\u001b[39m mpl\u001b[39m.\u001b[39mrcParams[\u001b[39m'\u001b[39m\u001b[39mpatch.facecolor\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    342\u001b[0m alpha \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_alpha \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fill \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 343\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_facecolor \u001b[39m=\u001b[39m colors\u001b[39m.\u001b[39;49mto_rgba(color, alpha)\n\u001b[1;32m    344\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstale \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/ws.leonid/lectures/.venv/lib/python3.8/site-packages/matplotlib/colors.py:182\u001b[0m, in \u001b[0;36mto_rgba\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m rcParams\n\u001b[1;32m    181\u001b[0m     prop_cycler \u001b[39m=\u001b[39m rcParams[\u001b[39m'\u001b[39m\u001b[39maxes.prop_cycle\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m--> 182\u001b[0m     colors \u001b[39m=\u001b[39m prop_cycler\u001b[39m.\u001b[39;49mby_key()\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mcolor\u001b[39m\u001b[39m'\u001b[39m, [\u001b[39m'\u001b[39m\u001b[39mk\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    183\u001b[0m     c \u001b[39m=\u001b[39m colors[\u001b[39mint\u001b[39m(c[\u001b[39m1\u001b[39m:]) \u001b[39m%\u001b[39m \u001b[39mlen\u001b[39m(colors)]\n\u001b[1;32m    184\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/ws.leonid/lectures/.venv/lib/python3.8/site-packages/cycler.py:389\u001b[0m, in \u001b[0;36mCycler.by_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m out \u001b[39m=\u001b[39m {k: \u001b[39mlist\u001b[39m() \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m keys}\n\u001b[1;32m    388\u001b[0m \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 389\u001b[0m     \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m keys:\n\u001b[1;32m    390\u001b[0m         out[k]\u001b[39m.\u001b[39mappend(d[k])\n\u001b[1;32m    391\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1500 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_ft.eval()\n",
    "out_dir = Path('../outputs/caltech101_vis/')\n",
    "for image_index in tqdm(range(len(val_dataset_unchanged))):\n",
    "    image_unchanged, image_category = val_dataset_unchanged[image_index]\n",
    "    true_category_name = caltech101.categories[image_category]\n",
    "    # prepare image\n",
    "    image_transformed = torch.unsqueeze(transformer(image_unchanged), 0).to(device)\n",
    "    # get class scores\n",
    "    class_scores = model_ft(image_transformed)\n",
    "    class_scores = class_scores.detach().cpu().numpy()[0]\n",
    "    predicted_class = np.argmax(class_scores)\n",
    "\n",
    "    formated_image_index = str(image_index).zfill(4)\n",
    "\n",
    "    if predicted_class == image_category:\n",
    "        # right classified image. Save its visualization to foulder with class category name\n",
    "        \n",
    "        save_path =out_dir / Path(\n",
    "            f\"true/{true_category_name}_id_{image_category}/{formated_image_index}.png\"\n",
    "        )\n",
    "        get_explanations(\n",
    "            model_ft,\n",
    "            image_transformed,\n",
    "            image_unchanged,\n",
    "            true_category_name,\n",
    "            image_category,\n",
    "            save_path,\n",
    "        )\n",
    "    else:\n",
    "        # misclassified image. Save vis with respect to true and predicted classes\n",
    "        predicted_category_name = caltech101.categories[predicted_class]\n",
    "        predicted_class_score = str(round(class_scores[predicted_class], 3))\n",
    "        true_class_score = str(round(class_scores[image_category], 3))\n",
    "        save_path_predicted_vis = out_dir / Path(\n",
    "            f\"mis_class/{predicted_category_name}_id_{predicted_class}_predicted\"\n",
    "        )\n",
    "        save_path_predicted_vis/= Path(f\"{formated_image_index}_predicted_category({predicted_category_name})_score_{predicted_class_score}_vis.png\")\n",
    "\n",
    "        save_path_true_vis = out_dir / Path(\n",
    "            f\"mis_class/{predicted_category_name}_id_{predicted_class}_predicted\"\n",
    "        )\n",
    "        save_path_true_vis/=Path(f\"{formated_image_index}_true_category({true_category_name})_score_{true_class_score}_vis.png\")\n",
    "\n",
    "        # predicted target vis\n",
    "        get_explanations(\n",
    "            model_ft,\n",
    "            image_transformed,\n",
    "            image_unchanged,\n",
    "            true_category_name,\n",
    "            predicted_class,\n",
    "            save_path_predicted_vis,\n",
    "        )\n",
    "        # true target vis\n",
    "        get_explanations(\n",
    "            model_ft,\n",
    "            image_transformed,\n",
    "            image_unchanged,\n",
    "            true_category_name,\n",
    "            image_category,\n",
    "            save_path_true_vis,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'13.827'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(predicted_class_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for module_pos, module in model_ft.features._modules.items():\n",
    "#     print(module_pos, module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Linear(in_features=25088, out_features=4096, bias=True)\n",
      "1 ReLU(inplace=True)\n",
      "2 Dropout(p=0.5, inplace=False)\n",
      "3 Linear(in_features=4096, out_features=4096, bias=True)\n",
      "4 ReLU(inplace=True)\n",
      "5 Dropout(p=0.5, inplace=False)\n",
      "6 Linear(in_features=4096, out_features=101, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for module_pos, module in model_ft.classifier._modules.items():\n",
    "    print(module_pos, module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "              ReLU-2         [-1, 64, 224, 224]               0\n",
      "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
      "              ReLU-4         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
      "            Conv2d-6        [-1, 128, 112, 112]          73,856\n",
      "              ReLU-7        [-1, 128, 112, 112]               0\n",
      "            Conv2d-8        [-1, 128, 112, 112]         147,584\n",
      "              ReLU-9        [-1, 128, 112, 112]               0\n",
      "        MaxPool2d-10          [-1, 128, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]         295,168\n",
      "             ReLU-12          [-1, 256, 56, 56]               0\n",
      "           Conv2d-13          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-14          [-1, 256, 56, 56]               0\n",
      "           Conv2d-15          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-16          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-17          [-1, 256, 28, 28]               0\n",
      "           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n",
      "             ReLU-19          [-1, 512, 28, 28]               0\n",
      "           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-21          [-1, 512, 28, 28]               0\n",
      "           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-23          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-24          [-1, 512, 14, 14]               0\n",
      "           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-26          [-1, 512, 14, 14]               0\n",
      "           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-28          [-1, 512, 14, 14]               0\n",
      "           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-30          [-1, 512, 14, 14]               0\n",
      "        MaxPool2d-31            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n",
      "           Linear-33                 [-1, 4096]     102,764,544\n",
      "             ReLU-34                 [-1, 4096]               0\n",
      "          Dropout-35                 [-1, 4096]               0\n",
      "           Linear-36                 [-1, 4096]      16,781,312\n",
      "             ReLU-37                 [-1, 4096]               0\n",
      "          Dropout-38                 [-1, 4096]               0\n",
      "           Linear-39                  [-1, 101]         413,797\n",
      "================================================================\n",
      "Total params: 134,674,341\n",
      "Trainable params: 134,674,341\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 218.77\n",
      "Params size (MB): 513.74\n",
      "Estimated Total Size (MB): 733.09\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model_ft, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bada2b9662873f86490f179f527b07823fa53d955971105c0ff3ba75e58f0801"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
